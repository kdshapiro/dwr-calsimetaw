* CalSIMETAW Processing

** PRISM Data

*** Download the prism data

Since 2010 PRISM has supplied daily data for processing.  One issue
with this data is that they maintain a 6 month period where the data
is identified as _provisional_.  Sometimes, we don't like to wait, and
so we use some of the provisional data is input for the CIMIS data.

What data to try and download is determined by the location that you
are currently using.  Schema _prism-stable_ retrieves stable data
while _prism-provisional_ retrieves provisonal data.

Either way the process is the same, we use the prism/daily.mk script
to fetch that data.  You need to be executing in grass for this to
work.

#+BEGIN_SRC bash
start=2017-01-01
for d in `seq 0 365`; do
  m=`date --date="${start} + $d days" +%Y%m%d`;
  g.mapset -c $m;
  make -f daily.mk tmin tmax ppt;
done
#+END_SRC


*** Reproject to CIMIS

There currently is no makefile to do this.  We just do that by
scripting, which is a bit dumb but there you are.  In the
ca-daily-prism code, you use the following.  Note that the make script
runs the calsimetaw method for ET.

#+BEGIN_SRC bash
start=2017-01-01
for d in `seq 0 365`; do
  m=`date --date="${start} + $d days" +%Y%m%d`;
  g.mapset -c $m;
  make -f calsimetaw.mk ETo ppt rf
done
#+END_SRC

** CalSIMETAW input csv files

The CalSIMETAW program basically runs it's processing on a per pixel basis, and
as such, they request weather inputs organized by pixel.  We use postgresql to
convert from the per image organization to the per pixel organization.

#+BEGIN_SRC bash
cimis_host=cimis.casil.ucdavis.edu
cimis_gdb=/home/quinn/gdb/cimis/quinn
cimis_bin=~/bin/get.one.year

prism_host=localhost
prism_gdb=/home/quinn/dwr-calsimetaw/gdb/prism
prism_bin=~/bin/get.one.prism.water.year

year=2016;
for d in prism cimis; do
  eval host=\$${d}_host;
  eval mapset=\$${d}_mapset;
  eval bin=\$${d}_bin;
  ssh -t quinn@${host} "year=$year GRASS_BATCH_JOB=$bin grass ${mapset}" |\
    tail -n +5 | head -n-4 > ${d}_wy${year}.csv
done
#+END_SRC

This method executes a grass batch file on the Spatial CIMIS server, and the
server that contains the prism data.  These are pretty simple files, and
alternatively, you could run those scripts directly from grass.

#+BEGIN_SRC bash
#! /bin/bash
# This is for a calendar year
#run with export GRASS_BATCH_JOB=~/bin/get.one.year; year=2016 grass /home/quinn/gdb/cimis/quinn

g.region rast=state@4km
r.mask -o state@4km
# These are the parameters for PRISM data
declare -a vars=('tmin' 'tmax' 'ppt' 'ETo' 'rf')
# If you are extracting CIMIS data, there are more data to retrieve
# declare -a vars=('Tn' 'Tx' 'Tdew' 'U2' 'ETo' 'K' 'Rnl' 'Rs')


printf -v head "%s," "${vars[@]}"
head="date,x,y,${head%?}"    # Remove the final character of head

echo $head
for m in $(g.mapsets -l fs=newline | grep ^${year} ); do
    printf -v dvars "%s@${m}," "${vars[@]}";
    dvars=${dvars%?};
    r.out.xyz --q fs=',' input=$dvars | perl -p -e "s/^/$m,/";
done

#+END_SRC

#+BEGIN_SRC bash

#! /bin/bash
# This example is for a water year.
#run with export GRASS_BATCH_JOB=~/bin/get.one.year; year=2010 grass
/home/quinn/gdb/cimis/quinn
let b_year=${year}-1;

g.region rast=MASK@4km
r.mask -o MASK@4km
declare -a vars=('Tn' 'Tx' 'Tdew' 'U2' 'ETo' 'K' 'Rnl' 'Rs')
#declare -a vars=('Tn' 'Tx' 'ETo' )
printf -v head "%s," "${vars[@]}"
head="date,x,y,${head%?}"    # Remove the final character of head

echo $head
for m in $(g.mapsets -l fs=newline | grep ^${b_year}-1.-..; g.mapsets -l
fs=newline | grep ^${year}-0.-..); do
    printf -v dvars "%s@${m}," "${vars[@]}";
	    dvars=${dvars%?};
		    r.out.xyz --q fs=',' input=$dvars | perl -p -e "s/^/$m,/";
			done
#+END_SRC

The postgresql processing uploads the above data into a temporary
file, and then reformats it to a pixel oriented format.  This format
stores arrays of values by water year.  This is in the hopes that
eventually we will have the ability to make a more general API for
per-point access to this data.  The database I use is typically ~eto~.

The PRISM data is calculated and exported first.

#+BEGIN_SRC sql
\set year 2017
set search_path=prism,public;
delete from prism_wy where water_year=:year
select * from prism.create_by_day(:year,'/csv/prism_wy'||:'year'||'.csv');
select * from prism.add_to_wy(:year);
-- Now we've updated the table prism_wy for the new water year
-- Export to csv files.
select * from prism.out_wy('/csv/prism_2017',:year);
#+END_SRC

This is followed with the CIMIS data.  Note that CIMIS data needs to
follow prism data since it uses the PRISM precipitation data.  In the
cimis_add_to_wy() function.

#+BEGIN_SRC sql
\set pwd `pwd`
\set year 2016
set search_path=cimis,public;
delete from cimis_wy where year=:year
select * from cimis_create_by_day(:year,:'pwd'||'/cimis_wy'||:'year'||'.csv');
select * from cimis_add_to_wy(:year);
-- Now we've updated the table cimis_wy for the new water year
-- Export to csv files.
select * from cimis_out_wy(:'pwd'||'/cimis_wy2016',:year);
#+END_SRC

*** CIMIS ET test

In order to test some CIMIS ET calculations, I also needed our grass
ETo calcuations.  I made another outputer, so I could run:

#+BEGIN_SRC sql
\set pwd `pwd`
\set year 2015
set search_path=cimis,public;
select * from et_out_wy(:'pwd'||'/cimis_et_wy2015',:year);
#+END_SRC

** Patching
   
*** 2018-02-23

Some of the data that we need do not exist in the the UC Davis site, in
that case, we need to get the data from DWR.  The way that you can do that
is by getting the data from DWR via the http interface First, you need to
download the data, for example this is how we can 

#+BEGIN_SRC bash
date=2018/02/23
for f in ETo K Rnl Rs Rso Tdew Tn Tx U2; do
  http https://spatialcimis.water.ca.gov/cimis/${date}/${f}.asc.gz > ${f}.asc.gz
done
#+END_SRC

And the from that we can update a 'patch' mapset with this data. 

#+BEGIN_SRC bash
# grass /data/patch/quinn
date=20180223
g.mapset -c $date
for i in *.asc; do 
  n=$(basename $i .asc); r.in.gdal -o input=$i output=$n; 
done
#+END_SRC

And from then we can get a CSV file that we can use to patch the CIMIS data.

#+BEGIN_SRC bash
g.region rast=state@4km
r.mask -o state@4km
declare -a vars=('Tn' 'Tx' 'Tdew' 'U2' 'ETo' 'K' 'Rnl' 'Rs')
patch=20180223

for m in $patch; do
    printf -v dvars "%s@${m}," "${vars[@]}";
    dvars=${dvars%?};
    r.out.xyz --q seperator=',' input=$dvars | perl -p -e "s/^/$m,/";
done > 2018_patch.csv

#+END_SRC

Then we can add that to our cimis data so that we can use this in our postgres data.

#+BEGIN_SRC bash
cat 2018_patch.csv >> cimis_2018.csv
#+END_SRC

Now we want to rebuild this for the new data. We first need to get the
prism data back into the system.  Then

#+BEGIN_SRC sql
\set year 2018
set search_path=prism,public;
delete from prism.year where year=:year;
select * from prism.create_by_day(:year,'/csv/prism_'||:'year'||'.csv');
select * from prism.add_to_year(:year);
-- Export to csv files.
select * from prism.out_year('/csv/prism_2018',:year);
#+END_SRC

Next, we can look at the CIMIS data. We do this second, because we need the
ppt data from the prism dataset.

#+BEGIN_SRC sql
\set year 2018
set search_path=cimis,public;
delete from cimis.year where year=:year;
select * from cimis.create_by_day(:year,'/csv/cimis_'||:'year'||'.csv');
select * from cimis.add_to_year(:year);
-- Export to csv files.
select * from cimis.out_year('/csv/cimis_2018',:year);

#+END_SRC

In order to copy data from the new site, I used the following steps to create a new mask.

#+BEGIN_SRC bash
g.region rast=state@2km
g.region res=4000
r.mapcalc expression='state_4km=state@2km'
r.mask state_4km@quinn
#+END_SRC

Then you can get data like this. In this case, I've only got data from after 2018-10-21.

#+BEGIN_SRC bash
declare -a vars=('Tn' 'Tx' 'Tdew' 'U2' 'ETo' 'K' 'Rnl' 'Rs')
printf -v head "%s," "${vars[@]}"
head="date,x,y,${head%?}" 
echo $head > 2018-10+.csv
for m in $(g.mapsets -l separator=newline | grep '^201810[23].$');do 
 echo $m; 
 printf -v dvars "%s@${m}," "${vars[@]}"; 
 dvars=${dvars%?}; 
 r.out.xyz --q separator=',' input=$dvars | perl -p -e "s/^/$m,/" >> 2018-10+.csv; 
done
# And now Nov, Dec
for m in $(g.mapsets -l separator=newline | grep '^20181[12]..$');do 
 echo $m; 
 printf -v dvars "%s@${m}," "${vars[@]}"; 
 dvars=${dvars%?}; 
 r.out.xyz --q separator=',' input=$dvars | perl -p -e "s/^/$m,/" >> 2018-10+.csv; 
done

#+END_SRC

*** 2019

For 2019, there were many dates missing on UCD site; 20190426 20191202
20191209 20191211 20191212 20191213 20191214 20191215 20191216 20191217
20191218 20191219 20191220 20191221 20191222 20191223 20191224 20191225 20191226 20191227
20191228 20191229 20191230 20191231

 Some of the data that we need do not exist in the the UC Davis site, in
that case, we need to get the data from DWR.  The way that you can do that
is by getting the data from DWR via the http interface First, you need to
download the data, for example this is how we can

#+BEGIN_SRC bash
dates='0426 1202 1209 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231'

for i in $dates; do 
  echo $i
  d=$(sed -e 's|\(..\)\(..\)|\1/\2|' <<< $i)
  [[ -d 2019${i} ]] || mkdir 2019$i;
  for f in ETo K Rnl Rs Rso Tdew Tn Tx U2; do
    http https://spatialcimis.water.ca.gov/cimis/2019/${d}/${f}.asc.gz > 2019${i}/${f}.asc.gz
  done
done

#+END_SRC

However, even DWR missing data from these days.  The days with no data
whatsoever are: dates='0426 1224 1225 1226'.  For these, we'll copy data
from 0425,1223,1223, and 1227 respectively.

#+BEGIN_SRC bash
for i in 0425; do 
  echo $i
  d=$(sed -e 's|\(..\)\(..\)|\1/\2|' <<< $i)
  [[ -d 2019${i} ]] || mkdir 2019$i;
  for f in ETo K Rnl Rs Rso Tdew Tn Tx U2; do
    http https://spatialcimis.water.ca.gov/cimis/2019/${d}/${f}.asc.gz > 2019${i}/${f}.asc.gz
  done
done

rsync -a 20190425/ 20190426/
rsync -a 20191223/ 20191224/
rsync -a 20191223/ 20191225/
rsync -a 20191227/ 20191226/

#+END_SRC

And the from that we can update a 'patch' mapset with this data. 

#+BEGIN_SRC bash
# grass /data/patch/quinn
dates='0426 1202 1209 1211 1212 1213 1214 1215 1216 1217 1218 '
dates+='1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231'

for d in $dates; do
 date=2019${d};
 echo $date;
 g.mapset -c $date
 for i in ${date}/*.asc; do 
   n=$(basename $i .asc); r.in.gdal -o input=$i output=$n; 
 done
done
#+END_SRC

And now, again we can make a patch CSV file for these data.

#+BEGIN_SRC bash
g.region rast=state@4km
r.mask state@4km
declare -a vars=('Tn' 'Tx' 'Tdew' 'U2' 'ETo' 'K' 'Rnl' 'Rs')

dates='0426 1202 1209 1211 1212 1213 1214 1215 1216 1217 1218 '
dates+='1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231'

rm 2019_patch.csv
for d in $dates; do
    m=2019$d;
    echo $m
    printf -v dvars "%s@${m}," "${vars[@]}";
    dvars=${dvars%?};
    r.out.xyz --q separator=',' input=$dvars | perl -p -e "s/^/$m,/" >> 2019_patch.csv
done 

#+END_SRC

Then we can add that to our cimis data so that we can use this in our postgres data.

#+BEGIN_SRC bash
cat 2019_patch.csv >> ../csv/cimis_2019.csv
#+END_SRC

Now we want to rebuild this for the new data. We first need to get the
prism data back into the system.  Then

#+BEGIN_SRC sql
\set year 2019
set search_path=prism,public;
delete from prism.year where year=:year;
select * from prism.create_by_day(:year,'/csv/prism_'||:'year'||'.csv');
select * from prism.add_to_year(:year);
-- Export to csv files.
select * from prism.out_year('/csv/prism_2019',:year);
#+END_SRC

Next, we can look at the CIMIS data. We do this second, because we need the
ppt data from the prism dataset.

#+BEGIN_SRC sql
\set year 2019
set search_path=cimis,public;
delete from cimis.year where year=:year;
select * from cimis.create_by_day(:year,'/csv/cimis_'||:'year'||'.csv');
select * from cimis.add_to_year(:year);
-- Export to csv files.
select * from cimis.out_year('/csv/cimis_2019',:year);

#+END_SRC

